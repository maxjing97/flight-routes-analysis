





import requests
from bs4 import BeautifulSoup
import csv
import pandas as pd
# Suppress just SettingWithCopyWarningda
import warnings
warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)





# Example: load HTML from a URL or string
url = "https://gettocenter.com/airports/top-100-airports-in-world/1000"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

table = soup.find("table")  
rows = table.find_all("tr")

with open("./data/top_airports_basic_data.csv", "w", newline="", encoding="utf-8") as f:
    writer = csv.writer(f)
    for row in rows:
        # Extract all cells (td or th)
        cols = row.find_all(["td", "th"])
        # Write the row text content to CSV
        writer.writerow([col.get_text(strip=True) for col in cols])
    f.close()





data = pd.read_csv("./data/top_airports_basic_data.csv", names=["full_name", "iata", "city", "country", "estimated_pax"])
print(len(data))
data=data.dropna()
print(len(data))
data.head(n=11)





data.to_csv("./data/top_airports_basic_data.csv")





#testing a function
def get_wikipedia_url_from_name(name):
    """Search Wikipedia using IATA code and return the best-matching article title."""
    search_url = "https://en.wikipedia.org/w/api.php"
    params = {
        "action": "query",
        "list": "search",
        "srsearch": f"{name}",
        "format": "json"
    }

    response = requests.get(search_url, params=params)
    data = response.json()
    try:
        raw_name = data['query']['search'][0]['title'] 
        raw_name = raw_name.replace(" ", "_") #replace the raw name spaces with _

        return "https://en.wikipedia.org/wiki/"+raw_name #format for english wiki
    except (KeyError, IndexError):
        return None
    
print(get_wikipedia_url_from_name("Hartsfieldâ€“Jackson Atlanta International Airport")) #popular airport
print(get_wikipedia_url_from_name("Gobernador Castello Airport")) #more obscure airport






data = pd.read_csv("./data/top_airports_basic_data.csv")


data["wiki_url"] =None #add column
for index, row in data.iterrows():
    try:
        print("current index:", index)
        name = row["full_name"]
        url = get_wikipedia_url_from_name(name)
        data["wiki_url"][index] = url
    except:
        data["wiki_url"][index] = None


data.head(n=989)





print(f"Number of null entries: {data['wiki_url'].isnull().sum()}")





data[data['wiki_url'].isnull()]





data["wiki_url"][97] = "https://en.wikipedia.org/wiki/Qingdao_Liuting_International_Airport"
data["wiki_url"][98] = "https://en.wikipedia.org/wiki/Brisbane_Airport"
data["wiki_url"][138] = "https://en.wikipedia.org/wiki/Bras%C3%ADlia_International_Airport"
data["wiki_url"][161] = "https://en.wikipedia.org/wiki/Nice_C%C3%B4te_d%27Azur_Airport"
data["wiki_url"][671] = "https://en.wikipedia.org/wiki/Maring%C3%A1_Regional_Airport"
data["wiki_url"][677] = "https://en.wikipedia.org/wiki/Lajes_Field"
data["wiki_url"][972] = "https://en.wikipedia.org/wiki/San_Rafael_Airport_(Argentina)"






data.to_csv("./data/top_airports_basic_data.csv")











#the wikipedia name based on the url, is simply found as:
print("name:", "https://en.wikipedia.org/wiki/Victoria_International_Airport".split("https://en.wikipedia.org/wiki/")[1])


data = pd.read_csv("./data/top_airports_basic_data.csv")
data.head(n=1)


data["wiki_name"] = None 
wiki_names = []

for i, row in data.iterrows():
    url = str(row["wiki_url"])
    wiki_names.append(url.split("https://en.wikipedia.org/wiki/")[1])
    
data["wiki_name"] = wiki_names
#overwrite
data.to_csv("./data/top_airports_basic_data.csv")








def get_destinations(iata_source, wiki_name):
    url = f"https://en.wikipedia.org/wiki/{wiki_name}"
    response = requests.get(url)
    
    soup = BeautifulSoup(response.text, 'html.parser')
    #find the related destination table
    heading = soup.find("h2", string="Airlines and destinations")
    table = heading.find_next("table") 
    while ('wikitable' not in table.get("class")): #find the next table matching a predictable class, if one has not been found
        table = table.find_next("table") 
    rows = table.find_all("tr")

    
    for i in range(1,len(rows)): #exclude the first row
        row = rows[i]
        # Extract all cells (td or th)
        cols = row.find_all(["td", "th"])
        # Write the row text content to CSV
        #first column is the airline
        airline = cols[0].get_text(strip=True)
        print("current airline:", airline)
        #get the list of destinations in the 2nd  
        destinations = cols[1]
        isSeasonal = 0 #iterate over subcomponents (seasonal always comes last, so set is seasonal to be false for now)
        for child in destinations.children: 
            #anchor components are the only destinations
            if (child.name == "a"):
                dest_name = child.get('title') #the title is the official wikipedia airport name (without _ in place of spaces)
                dest_name = dest_name.replace(" ", "_") 
                output = f"{iata_source},{wiki_name},{dest_name},{airline},{isSeasonal}\n" #final output to append to the file
                print("DESTINATION FOUND:", output)
            elif ((child.name == "b") and (child.text == "Seasonal:")):
                isSeasonal = 1 #get seasonal to be 1 for future destinations
            






get_destinations("JFK","John_F._Kennedy_International_Airport")



get_destinations("DFW","Dallas_Fort_Worth_International_Airport")


get_destinations("OUI","Ushant_Airport")


get_destinations("CAN", "Guangzhou_Baiyun_International_Airport")


get_destinations("VDM","Gobernador_Edgardo_Castello_Airport")



get_destinations("FRA","Frankfurt_Airport")


get_destinations("PAP","Toussaint_Louverture_International_Airport")


get_destinations("PEK","Beijing_Capital_International_Airport")



get_destinations("DEL","Indira_Gandhi_International_Airport")


import sys
print(sys.executable)
