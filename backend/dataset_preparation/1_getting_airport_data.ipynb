{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c7ef589",
   "metadata": {},
   "source": [
    "\n",
    "# General information of the datascrapping process.\n",
    "\n",
    "Here, we get vital data for the key airports we want to scrape data from:\n",
    "\n",
    "location (coordinates, country, city), IATA code,\n",
    "\n",
    "This then allows us, using the wikipedia API to find each airport's corresponding wikipedia link and wikipedia name.\n",
    "For example JFK is encoded in wikipedia's internal database as John_F._Kennedy_International_Airport. This matches the last part of the url in the wikipedia page link for the airport:     \n",
    "https://en.wikipedia.org/wiki/John_F._Kennedy_International_Airport\n",
    "\n",
    "For each time range:\n",
    "\n",
    "The wikipedia page's raw text can be scrapped predictiably for a list of destinations from an airport. We will encode airport destinations using iata code.\n",
    "\n",
    "We do this for each airport, generating a large csv file of airport-destination pairs for a particular time change.\n",
    "\n",
    "We will look at 2 time ranges (now(as of June 4th 11am Eastern Time), before Jan 1, 2000 UTC 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a557d",
   "metadata": {},
   "source": [
    "### Sources :\n",
    "\n",
    "\n",
    "list of top 1000 airports by traffic to scrape:\n",
    "\n",
    "https://gettocenter.com/airports/top-100-airports-in-world/1000#google_vignette  \n",
    "\n",
    "\n",
    "detailed airport database to cross reference:\n",
    "\n",
    "https://www.partow.net/miscellaneous/airportdatabase/index.html#Downloads "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4779b1bb",
   "metadata": {},
   "source": [
    "## Part 1: get data for the list of top 1000 airports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad92860",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mBad address (C:\\ci\\zeromq_1616055400030\\work\\src\\epoll.cpp:100). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25fa4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example: load HTML from a URL or string\n",
    "url = \"https://gettocenter.com/airports/top-100-airports-in-world/1000\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "table = soup.find(\"table\")  \n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "with open(\"top1000_raw_data.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in rows:\n",
    "        # Extract all cells (td or th)\n",
    "        cols = row.find_all([\"td\", \"th\"])\n",
    "        # Write the row text content to CSV\n",
    "        writer.writerow([col.get_text(strip=True) for col in cols])\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
